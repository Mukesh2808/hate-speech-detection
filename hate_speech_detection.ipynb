{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTzvfUbYh4agEkSd7haTfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mukesh2808/hate-speech-detection/blob/main/hate_speech_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # read the csv\n",
        "import re # regex to detect username, url, html entity\n",
        "import nltk # to use word tokenize (split the sentence into words)\n",
        "from nltk.corpus import stopwords # to remove the stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as K"
      ],
      "metadata": {
        "id": "DbjMFYMQBczY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/train.csv\")\n",
        "\n",
        "data.head()"
      ],
      "metadata": {
        "id": "FdoKHwsqBnZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset shape to know how many tweets in the datasets\n",
        "print(f\"num of tweets: {data.shape}\")\n",
        "\n",
        "# extract the text and labels\n",
        "tweet = list(data['tweet'])\n",
        "labels = list(data['class'])"
      ],
      "metadata": {
        "id": "pJ0DzkXuB0n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n"
      ],
      "metadata": {
        "id": "hw5BAeV9Cph4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Install missing package\n",
        "!pip install emoji --quiet\n",
        "\n",
        "# ‚úÖ Import and download NLTK resources\n",
        "import re\n",
        "import nltk\n",
        "import emoji\n",
        "nltk.download('punkt', force=True)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# ‚úÖ Initialize stopwords and add 'rt' (retweet)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.add(\"rt\")\n",
        "\n",
        "# ‚úÖ Remove HTML entities\n",
        "def remove_entity(raw_text):\n",
        "    entity_regex = r\"&[^\\s;]+;\"\n",
        "    return re.sub(entity_regex, \"\", raw_text)\n",
        "\n",
        "# ‚úÖ Replace @mentions with \"user\"\n",
        "def change_user(raw_text):\n",
        "    # Ensure raw_text is a string before using re.sub\n",
        "    if not isinstance(raw_text, str):\n",
        "        return raw_text # Or handle the error appropriately\n",
        "    return re.sub(r\"@(\\w+)\", \"user\", raw_text)\n",
        "\n",
        "# ‚úÖ Remove URLs\n",
        "def remove_url(raw_text):\n",
        "    # Ensure raw_text is a string before using re.sub\n",
        "    if not isinstance(raw_text, str):\n",
        "        return raw_text # Or handle the error appropriately\n",
        "    url_regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?¬´¬ª‚Äú‚Äù‚Äò‚Äô]))\"\n",
        "    return re.sub(url_regex, '', raw_text)\n",
        "\n",
        "# ‚úÖ Remove unwanted punctuation/symbols\n",
        "def remove_noise_symbols(raw_text):\n",
        "    # Ensure raw_text is a string before using string methods\n",
        "    if not isinstance(raw_text, str):\n",
        "        return raw_text # Or handle the error appropriately\n",
        "    text = raw_text.replace('\"', '').replace(\"'\", '')\n",
        "    text = text.replace(\"!\", '').replace(\"`\", '').replace(\"..\", '')\n",
        "    return text\n",
        "\n",
        "# ‚úÖ Remove emojis\n",
        "def remove_emojis(raw_text):\n",
        "     # Ensure raw_text is a string before using emoji.replace_emoji\n",
        "    if not isinstance(raw_text, str):\n",
        "        return raw_text # Or handle the error appropriately\n",
        "    return emoji.replace_emoji(raw_text, replace='')\n",
        "\n",
        "# ‚úÖ Remove stopwords\n",
        "def remove_stopwords(raw_text):\n",
        "     # Ensure raw_text is a string before using word_tokenize\n",
        "    if not isinstance(raw_text, str):\n",
        "        return raw_text # Or handle the error appropriately\n",
        "    tokenize = word_tokenize(raw_text)\n",
        "    text = [word for word in tokenize if word.lower() not in stop_words]\n",
        "    return \" \".join(text)\n",
        "\n",
        "\n",
        "# ‚úÖ Full preprocessing pipeline (list of texts)\n",
        "def preprocess(text_list):\n",
        "    print(f\"Input list length to preprocess: {len(text_list)}\")\n",
        "    clean = []\n",
        "    for i, item in enumerate(text_list):\n",
        "        # Explicitly cast to string to ensure correct type\n",
        "        text = str(item)\n",
        "        # print(f\"Processing item at index {i}, type: {type(text)}\")\n",
        "        try:\n",
        "            text = change_user(text)\n",
        "            # print(f\"After change_user (index {i}): {text}\")\n",
        "            text = remove_entity(text)\n",
        "            # print(f\"After remove_entity (index {i}): {text}\")\n",
        "            text = remove_url(text)\n",
        "            # print(f\"After remove_url (index {i}): {text}\")\n",
        "            text = remove_noise_symbols(text)\n",
        "            # print(f\"After remove_noise_symbols (index {i}): {text}\")\n",
        "            text = remove_emojis(text)\n",
        "            # print(f\"After remove_emojis (index {i}): {text}\")\n",
        "            text = remove_stopwords(text)\n",
        "            # print(f\"After remove_stopwords (index {i}): {text}\")\n",
        "            clean.append(text)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing item at index {i}: {item}. Error: {e}\")\n",
        "            # Decide how to handle the error - skip, append original, etc.\n",
        "            # For now, let's skip the item if an error occurs\n",
        "            continue\n",
        "\n",
        "    print(f\"Output list length from preprocess: {len(clean)}\")\n",
        "    return clean"
      ],
      "metadata": {
        "id": "ItUMOmnXB4Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /root/nltk_data/tokenizers/punkt\n"
      ],
      "metadata": {
        "id": "H1gg8LU8DdJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt', force=True)\n"
      ],
      "metadata": {
        "id": "iSqdYiUXDido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt', force=True) # Download the correct resource\n",
        "\n",
        "tweet = \"RT @user: I hate this! üò°\"\n",
        "clean_tweet = preprocess([tweet])[0]"
      ],
      "metadata": {
        "id": "d4uGUArrEDsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "051cf452"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt', force=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "j5AQXFqcFY0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Peek at the dataset to check column names\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3xedA_PBFdnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "2mvkUaptF5Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df['class'].tolist()\n"
      ],
      "metadata": {
        "id": "qCWVU6jdGG_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_texts = preprocess(df['tweet'].tolist())\n",
        "\n",
        "# Change this if column is 'class' or something else\n",
        "labels = df['class'].tolist()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(clean_texts, labels, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "zvwu96ujGIfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "# build the vocabulary based on train dataset\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# tokenize the train and test dataset\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# vocabulary size (num of unique words) -> will be used in embedding layer\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "yQ64IB42GQQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(seq) for seq in X_train)\n",
        "\n",
        "# to test an outlier case (if one of the test dataset has longer length)\n",
        "for x in X_test:\n",
        "    if len(x) > max_length:\n",
        "        print(f\"an outlier detected: {x}\")\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = max_length)\n",
        "X_test = pad_sequences(X_test, maxlen = max_length)"
      ],
      "metadata": {
        "id": "Pyfzz_YGGXUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = to_categorical(y_test, num_classes=3)\n",
        "y_train = to_categorical(y_train, num_classes=3)"
      ],
      "metadata": {
        "id": "6p-AMdksGdcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"num test tweet: {y_test.shape[0]}\")\n",
        "print(f\"num train tweet: {y_train.shape[0]}\")"
      ],
      "metadata": {
        "id": "ma0g-SwPGgcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    precisions = precision(y_true, y_pred)\n",
        "    recalls = recall(y_true, y_pred)\n",
        "    return 2*((precisions*recalls)/(precisions+recalls+K.epsilon()))"
      ],
      "metadata": {
        "id": "is20roqWGkvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 200\n",
        "\n",
        "# LSTM model architechture (CNN + LSTM)\n",
        "model = Sequential([\n",
        "    # embedding layer is like idk\n",
        "    Embedding(vocab_size, output_dim, input_length=max_length),\n",
        "    # lstm for xxx\n",
        "    LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
        "    # dropout to prevent overfitting\n",
        "    Dropout(0.5),\n",
        "    # dense to connect the previous output with current layer\n",
        "    Dense(128, activation=\"relu\"),\n",
        "    # dropout to prevent overfitting\n",
        "    Dropout(0.5),\n",
        "    # this is output layer, with 3 class (0, 1, 2)\n",
        "    Dense(3, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy',f1,precision, recall])"
      ],
      "metadata": {
        "id": "S19CkqizGmWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the model parameters\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "v9slimCzGxMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K  # ‚úÖ correct\n"
      ],
      "metadata": {
        "id": "rwpEV_56OUqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import backend as K  # ‚úÖ preferred\n"
      ],
      "metadata": {
        "id": "Uk2uszSKOfGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n"
      ],
      "metadata": {
        "id": "X4Spm9v4OkUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the texts\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_texts)\n",
        "sequences = tokenizer.texts_to_sequences(clean_texts)\n",
        "\n",
        "# Pad the sequences\n",
        "maxlen = 100  # or pick a suitable maxlen\n",
        "X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Convert labels to numpy array\n",
        "y = np.array(labels)\n",
        "\n",
        "# Now split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n"
      ],
      "metadata": {
        "id": "eL5OrzTPPe15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train type:\", type(X_train))\n",
        "print(\"X_train shape:\", getattr(X_train, 'shape', 'Not available'))\n",
        "print(\"Sample X_train:\", X_train[0] if len(X_train) > 0 else \"X_train is empty!\")\n",
        "\n",
        "print(\"y_train type:\", type(y_train))\n",
        "print(\"y_train shape:\", getattr(y_train, 'shape', 'Not available'))\n",
        "print(\"Sample y_train:\", y_train[0] if len(y_train) > 0 else \"y_train is empty!\")\n"
      ],
      "metadata": {
        "id": "f1PN6KT_Pu21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Print column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "TCslj5JVRJaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
        "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(tokens)\n"
      ],
      "metadata": {
        "id": "YKc000YgRKfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# Replace with actual column names\n",
        "df['cleaned_text'] = df['tweet'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "7CPVgTiTRNM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "id": "HjGBPLIzRvHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(df['class'])  # Use the correct column name\n"
      ],
      "metadata": {
        "id": "AnyexKFnSXW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Features and Labels\n",
        "X = df['cleaned_text']\n",
        "y = to_categorical(df['class'])  # <-- Update this\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_vec = vectorizer.fit_transform(X).toarray()\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "y3w-2rNUSYik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))  # use 3 if you have 3 classes\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "30WZttjHTQRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n"
      ],
      "metadata": {
        "id": "g1kVKCQDTYQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "hist = model.history.history\n",
        "plt.plot(hist['loss'],'r',linewidth=2, label='Training loss')\n",
        "plt.plot(hist['val_loss'], 'g',linewidth=2, label='Validation loss')\n",
        "plt.title('Hate Speech and Offensive language Model')\n",
        "plt.xlabel('Epochs numbers')\n",
        "plt.ylabel('MSE numbers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2n0LrADiUHfj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}